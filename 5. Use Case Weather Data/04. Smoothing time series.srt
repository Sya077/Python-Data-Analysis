1
00:00:00,05 --> 00:00:02,01
- [Instructor] Now we know how to load

2
00:00:02,01 --> 00:00:04,05
temperature data from any station,

3
00:00:04,05 --> 00:00:06,02
how to compute basic summaries

4
00:00:06,02 --> 00:00:08,08
such as mean, min and max,

5
00:00:08,08 --> 00:00:11,06
and how to integrate missing data points

6
00:00:11,06 --> 00:00:13,03
using interpolation.

7
00:00:13,03 --> 00:00:16,08
We'll continue with more data analysis in NumPy.

8
00:00:16,08 --> 00:00:20,02
I've copied your fill NaNs function here

9
00:00:20,02 --> 00:00:22,08
since we will need it.

10
00:00:22,08 --> 00:00:25,02
We looked at data for Pasadena in the last video,

11
00:00:25,02 --> 00:00:27,03
now let's move to even sunnier skies

12
00:00:27,03 --> 00:00:29,05
by looking at weather in the town of Hilo,

13
00:00:29,05 --> 00:00:31,07
big island Hawaii.

14
00:00:31,07 --> 00:00:33,09
We use our custom loader

15
00:00:33,09 --> 00:00:39,04
and again, I encourage you to go look under the hood.

16
00:00:39,04 --> 00:00:43,04
This is data in fact from Hilo International Airport

17
00:00:43,04 --> 00:00:47,08
we now fill the missing data for both T min and T max.

18
00:00:47,08 --> 00:00:53,01
Once more two pole unpacking is very useful.

19
00:00:53,01 --> 00:00:55,05
Let's look at some data summaries.

20
00:00:55,05 --> 00:00:57,09
The yearly average which gives us a sense

21
00:00:57,09 --> 00:01:02,00
of the typical value for T min, and it's min and max.

22
00:01:02,00 --> 00:01:06,00
Will span the range of variation of these measurements.

23
00:01:06,00 --> 00:01:08,08
We can plot the summaries together with the time series.

24
00:01:08,08 --> 00:01:12,05
The map load live function adds each line,

25
00:01:12,05 --> 00:01:16,03
plots a horizontal line that spans the entire graph.

26
00:01:16,03 --> 00:01:19,05
Also useful for reference values,

27
00:01:19,05 --> 00:01:23,07
and we'll make them dotted.

28
00:01:23,07 --> 00:01:26,00
Another common way to measure the range of variation

29
00:01:26,00 --> 00:01:29,04
of a time series is to compute the standard deviation

30
00:01:29,04 --> 00:01:32,02
defined at the square root of the values.

31
00:01:32,02 --> 00:01:34,08
If you don't know about this you can go to statistics,

32
00:01:34,08 --> 00:01:37,06
text book, or to Wikipedia.

33
00:01:37,06 --> 00:01:40,07
Mean and variance are computed in NumPy with NP Mean

34
00:01:40,07 --> 00:01:44,09
and NP Var, we can plot the time series again

35
00:01:44,09 --> 00:01:47,08
using the mean and the mean minus

36
00:01:47,08 --> 00:01:51,08
and plus the standard deviation as references.

37
00:01:51,08 --> 00:01:56,09
Most of the time the temperatures are included in this range

38
00:01:56,09 --> 00:01:59,03
and given that this is Hawaii it's also interesting

39
00:01:59,03 --> 00:02:02,08
to look at precipitation.

40
00:02:02,08 --> 00:02:10,02
We grab those values with Get Weather and just plot them.

41
00:02:10,02 --> 00:02:15,00
The rainy season starting in November is quite evident.

42
00:02:15,00 --> 00:02:17,07
Now looking at the data this way is very informative,

43
00:02:17,07 --> 00:02:20,07
but we also see lots of noise, rapid variations between

44
00:02:20,07 --> 00:02:25,08
one day and the next, which can obscure underlying trends.

45
00:02:25,08 --> 00:02:28,08
To remove the noise, we can smooth the data.

46
00:02:28,08 --> 00:02:31,03
So that we see the slower long-term behavior

47
00:02:31,03 --> 00:02:35,00
below those still issues and the simplest approach

48
00:02:35,00 --> 00:02:39,01
to smoothing is replacing each value with the average

49
00:02:39,01 --> 00:02:41,08
of a set of its neighbors.

50
00:02:41,08 --> 00:02:45,07
With NumPy we can do this with correlate.

51
00:02:45,07 --> 00:02:47,07
I will demonstrate first with a very simple

52
00:02:47,07 --> 00:02:51,01
and short data set consisting of two peaks

53
00:02:51,01 --> 00:02:54,09
over a background of zeroes.

54
00:02:54,09 --> 00:02:58,01
I then define a roughly triangular correlation mask

55
00:02:58,01 --> 00:03:02,09
which is highest in the middle and drops down to the sides

56
00:03:02,09 --> 00:03:06,09
and I have arranged the values so they sum to one,

57
00:03:06,09 --> 00:03:09,06
next I write the correlation.

58
00:03:09,06 --> 00:03:12,08
What it does is to multiply each element

59
00:03:12,08 --> 00:03:14,08
in the series with the mask

60
00:03:14,08 --> 00:03:18,05
and then sum up all the resulting short series.

61
00:03:18,05 --> 00:03:22,00
Sliding them so they are centered around the sample

62
00:03:22,00 --> 00:03:25,05
that we multiply, thus as you see in this figure,

63
00:03:25,05 --> 00:03:28,03
each peak generates a triangle centered

64
00:03:28,03 --> 00:03:31,01
on its location.

65
00:03:31,01 --> 00:03:33,08
I have not explained the keyword same that we gave

66
00:03:33,08 --> 00:03:37,08
to correlate, what it does is to request that

67
00:03:37,08 --> 00:03:40,03
the output of the correlation be the same length

68
00:03:40,03 --> 00:03:41,08
as the input.

69
00:03:41,08 --> 00:03:44,05
Even as this means that the points on the boundaries

70
00:03:44,05 --> 00:03:47,09
get sums from fewer masks.

71
00:03:47,09 --> 00:03:51,01
So we may see some anomaly there.

72
00:03:51,01 --> 00:03:54,02
To smooth our temperature series we will use

73
00:03:54,02 --> 00:03:58,06
an even simpler mask, uniform values normalized

74
00:03:58,06 --> 00:04:00,09
so that they sum to one.

75
00:04:00,09 --> 00:04:06,00
We're going to get that in NumPy with NumPy once,

76
00:04:06,00 --> 00:04:07,09
let's try this out.

77
00:04:07,09 --> 00:04:12,01
We plot the regional temperature series as dots,

78
00:04:12,01 --> 00:04:15,01
maybe a little smaller than usual,

79
00:04:15,01 --> 00:04:20,05
and the smoothed series has a continuous line.

80
00:04:20,05 --> 00:04:23,06
This works fine, we are reducing oscillations

81
00:04:23,06 --> 00:04:27,06
while emphasizing the underlying slower trend.

82
00:04:27,06 --> 00:04:29,06
We do see something strange happening

83
00:04:29,06 --> 00:04:32,04
at the beginning and end of the data set.

84
00:04:32,04 --> 00:04:35,00
The values are going way low.

85
00:04:35,00 --> 00:04:37,01
That has to do with the keyword same,

86
00:04:37,01 --> 00:04:40,05
and we detect that the values on the boundaries

87
00:04:40,05 --> 00:04:44,09
have fewer neighbors than everybody else.

88
00:04:44,09 --> 00:04:46,08
If we can do without a few points at the end

89
00:04:46,08 --> 00:04:49,03
and at the beginning we can change our request

90
00:04:49,03 --> 00:04:55,09
to amply correlate to valid and avoid this problem.

91
00:04:55,09 --> 00:04:58,06
We can now make a function to apply smoothing

92
00:04:58,06 --> 00:05:01,03
of any length to any array.

93
00:05:01,03 --> 00:05:05,00
The length of the smoothing mask is sometimes called window

94
00:05:05,00 --> 00:05:08,02
and by default we'll use valid mode.

95
00:05:08,02 --> 00:05:14,00
We then plot T min and T max together for Hilo.

96
00:05:14,00 --> 00:05:15,06
It's an interesting plot

97
00:05:15,06 --> 00:05:17,08
and I'd like to see it for other cities,

98
00:05:17,08 --> 00:05:19,09
so once again let's take a code

99
00:05:19,09 --> 00:05:24,02
and generalize it to arbitrary station and year.

100
00:05:24,02 --> 00:05:27,00
So we get the data, we fill in the NaNs

101
00:05:27,00 --> 00:05:31,08
and we plot both the original data and a smooth version.

102
00:05:31,08 --> 00:05:34,08
We need to offset the plot a bit since we lose points

103
00:05:34,08 --> 00:05:38,07
with valid mode, so the range if the smoothing is over

104
00:05:38,07 --> 00:05:41,04
20 days would just be from day 10

105
00:05:41,04 --> 00:05:45,07
to day 356 of the year.

106
00:05:45,07 --> 00:05:49,01
Let me try this out, for instance over multiple years

107
00:05:49,01 --> 00:05:55,00
for Hilo to see if the Hawaii climate is stable.

108
00:05:55,00 --> 00:05:56,08
Quite so.

109
00:05:56,08 --> 00:05:59,04
How about comparing cities in different climates?

110
00:05:59,04 --> 00:06:02,07
Let me look over Pasadena, New York, San Diego,

111
00:06:02,07 --> 00:06:07,02
and Minneapolis and then create a sub-plot

112
00:06:07,02 --> 00:06:12,09
in two rows and two columns for each one of them.

113
00:06:12,09 --> 00:06:18,01
I'll focus on the year 2000.

114
00:06:18,01 --> 00:06:20,08
There's some downloading and then we see the plots

115
00:06:20,08 --> 00:06:24,09
come out together, if you can choose based on weather alone

116
00:06:24,09 --> 00:06:26,00
where would you live?

